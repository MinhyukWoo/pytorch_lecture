{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `tensor` 탐구\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서의 속성\n",
    "\n",
    "- `shape`\n",
    "- `dtype`\n",
    "- `device`\n",
    "- `layout`\n",
    "- `memory_format`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `shape`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dtype`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhyuk/setup/miniconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])'s element data type is torch.float32\n",
      "tensor([4., 5., 6.])'s element data type is torch.float32\n",
      "tensor([7., 8., 9.])'s element data type is torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "float_tensor1 = torch.tensor([1, 2, 3], dtype=torch.float)\n",
    "float_tensor2 = torch.tensor([4, 5, 6], dtype=torch.float32)\n",
    "float_tensor3 = torch.FloatTensor([7, 8, 9])  # Legacy Constructors\n",
    "\n",
    "for tensor in [float_tensor1, float_tensor2, float_tensor3]:\n",
    "    print(f\"{tensor}'s element data type is {tensor.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.int32\n",
      "torch.bool\n",
      "torch.float64\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "# 32-bit floating point\n",
    "print(torch.float)\n",
    "# 32-bit integer (signed)\n",
    "print(torch.int)\n",
    "# Boolean\n",
    "print(torch.bool)\n",
    "\n",
    "# 64-bit floating point\n",
    "print(torch.double)\n",
    "# 64-bit inteber (signed)\n",
    "print(torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m])\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "torch.tensor([1, 2, 3]).dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `device`\n",
    "\n",
    "- cpu  \n",
    "  `\"cpu\"`\n",
    "- gpu  \n",
    "  `\"cuda\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])'s device is cpu\n",
      "tensor([4, 5, 6], device='cuda:0')'s device is cuda:0\n",
      "tensor([7, 8, 9], device='cuda:0')'s device is cuda:0\n"
     ]
    }
   ],
   "source": [
    "cpu_tensor = torch.tensor([1, 2, 3])\n",
    "gpu_tensor1 = torch.tensor([4, 5, 6], device=\"cuda\")\n",
    "gpu_tensor2 = torch.tensor([7, 8, 9], device=torch.device(\"cuda\"))\n",
    "\n",
    "for tensor in [cpu_tensor, gpu_tensor1, gpu_tensor2]:\n",
    "    print(f\"{tensor}'s device is {tensor.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "available_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(available_device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `layout`\n",
    "\n",
    "아직 베타 버전, 바뀔 수 있음\n",
    "\n",
    "> The `torch.layout` class is in beta and subject to change.\n",
    "\n",
    "- `torch.strided`: Dense Tensor\n",
    "- `torch.sparse_coo`: Sparse Tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `memory_format`\n",
    "\n",
    "- `torch.contiguous_format`\n",
    "- `torch.channels_last`\n",
    "- `torch.preserve_format`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서 생성하기\n",
    "\n",
    "- `torch.tensor()`  \n",
    "  인자로 전달된 array-like 데이터로 새로운 텐서를 반환한다.   \n",
    "- `torch.XXX()`  \n",
    "  인자로 전달된 size의 텐서를 새로 만든다.   \n",
    "- `torch.XXX_like()`  \n",
    "  인자로 전달된 텐서와 같은 size의 텐서를 새로 만든다.   \n",
    "  `dtype`, `layout`, `device`를 따로 설정하지 않으면 인자로 전달된 텐서와 같은 것을 따른다.   \n",
    "- `self.new_XXX()`  \n",
    "  인자로 size를 전달한다.  \n",
    "  `dtype`과 `device`를 유지하는 대신 새로운 텐서를 만든다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4.])\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data1 = torch.Tensor([1, 2, 3, 4])\n",
    "data2 = data1.new_zeros((3, 2))\n",
    "print(data1)\n",
    "print(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legacy Constructor\n",
    "\n",
    "- `torch.FloatTensor()`\n",
    "- `torch.cuda.FloatTensor()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "종류\n",
    "* `full`   \n",
    "매개변수 `size`와 `fill_value`\n",
    "* `empty`   \n",
    "매개변수 `size`\n",
    "* `ones`   \n",
    "매개변수 `size`   \n",
    "* `zeros`   \n",
    "매개변수 `size`   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `torch.tensor()`   \n",
    "항상 새롭게 복사된다. (deep copy)\n",
    "* `torch.asarray()` or `torch.as_tensor()`   \n",
    "조건에 따라서 복사될 수도, 메모리를 공유할 수도 있다.   \n",
    "`dtype`, `device`등을 새로 설정할 수 있다. (이럴 때 복사된다.)\n",
    "* `torch.from_numpy()`   \n",
    "항상 `ndarray`와 같은 메모리를 공유한다.(shallow copy)   \n",
    "`dtype`, `device`등을 바꿀 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "numpy_data = np.zeros((3, 4), dtype=float)\n",
    "tensor_data = torch.from_numpy(numpy_data)\n",
    "tensor_data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "        0.9000, 1.0000])\n",
      "tensor([1.0000e+00, 3.1623e+00, 1.0000e+01, 3.1623e+01, 1.0000e+02, 3.1623e+02,\n",
      "        1.0000e+03, 3.1623e+03, 1.0000e+04, 3.1623e+04, 1.0000e+05])\n"
     ]
    }
   ],
   "source": [
    "print(torch.arange(0, 10))\n",
    "print(torch.linspace(0, 1, 11))\n",
    "print(torch.logspace(0, 1, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연산\n",
    "\n",
    "`self.XXX`는 연산 결과가 반환되지만 self는 반영되지 않는다.  \n",
    "`self.XXX_`는 연산 결과가 in-place로 적용되고 반환도 된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inplace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Before===\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "===After===\n",
      "tensor([[12., 12., 12.],\n",
      "        [12., 12., 12.]])\n"
     ]
    }
   ],
   "source": [
    "data = torch.zeros(2, 3)\n",
    "print(\"===Before===\")\n",
    "print(data)\n",
    "data.add_(12)\n",
    "print(\"===After===\")\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape\n",
    "\n",
    "`view`  \n",
    "반드시 같은 데이터를 공유한다. (shallow copy)  \n",
    "조건에 따라서 RuntimeError을 띄우기도 한다.\n",
    "\n",
    "`reshape`  \n",
    "같은 데이터를 공유할 때도 있지만, 조건에 따라서 복사해서 제공할 때도 있다.  \n",
    "그러나 Copying과 viewing이 되는 상황을 구분해서 사용하는 것은 좋지 않다.\n",
    "\n",
    "> but you should not depend on the copying vs. viewing behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([1, 2, 3, 4])\n",
      "shape: torch.Size([1, 3, 2, 4])\n",
      "shape: torch.Size([1, 3, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "data = torch.arange(0, 24).reshape(1, 2, 3, 4)\n",
    "viewed_data = data.view(1, 3, 2, 4)\n",
    "reshaped_data = data.view(1, 3, 2, 4)\n",
    "\n",
    "for tensor in [data, viewed_data, reshaped_data]:\n",
    "    print(\"shape:\", tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n",
      "shape: torch.Size([4, 3, 2, 1])\n",
      "shape: torch.Size([1, 3, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "tranposed_data = torch.arange(0, 24).reshape(1, 2, 3, 4).T\n",
    "try:\n",
    "    viewed_data = tranposed_data.view(1, 3, 2, 4)\n",
    "except RuntimeError as error:\n",
    "    print(error)\n",
    "reshaped_data = tranposed_data.reshape(1, 3, 2, 4)\n",
    "\n",
    "reshaped_data.add_(10)\n",
    "for tensor in [tranposed_data, reshaped_data]:\n",
    "    print(\"shape:\", tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 3, 4]),)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = torch.arange(-2, 3)\n",
    "data2 = torch.zeros_like(data1)\n",
    "data1.argwhere()\n",
    "torch.where(data1 > 0, data1, data2)\n",
    "data1.nonzero()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`contiguous`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension swap\n",
    "\n",
    "- `transpose()`또는 `swapdims()`, `swapaxes()`  \n",
    "  두 차원끼리 교환할 때 사용한다. (shallow copy)\n",
    "- `permute()`  \n",
    "  원하는 차원들을 교환할 때 사용한다. (shallow copy)\n",
    "- `T`  \n",
    "  `permute(n-1, n-2, ..., 0)`과 동일하다.\n",
    "- `view()` and `reshape()`\n",
    "- `moveaxis` 또는 `movedim()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension insert\n",
    "\n",
    "* `unsqueeze`  \n",
    "* `view` and `reshape`  \n",
    "* `expand`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension reduction\n",
    "\n",
    "* `ravel`  \n",
    "* `flatten`\n",
    "* `squeeze`   \n",
    "size가 1인 차원을 제거한다.   \n",
    "입력된 차원의 size가 1이면 그 차원을 제거한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate\n",
    "\n",
    "* `torch.cat()` 또는 `torch.concat()`   \n",
    "붙이려는 차원의 수가 일치해야 한다.\n",
    "* `torch.stack()`\n",
    "* `vstack` and `row_stack`   \n",
    "Stack tensors in sequence vertically (row wise).\n",
    "* `hstack`   \n",
    "Stack tensors in sequence horizontally (column wise).\n",
    "* `column_stack`   \n",
    "Creates a new tensor by horizontally stacking the tensors in tensors.\n",
    "* `dstack`   \n",
    "Stack tensors in sequence depthwise (along third axis).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 2])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = torch.tensor([1, 2, 3])\n",
    "data2 = torch.tensor([4, 5, 6])\n",
    "torch.dstack((data1, data2)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split\n",
    "* `tensor_split`   \n",
    "정수거나 스칼라면 입력된 인자만큼의 section개로 나눈다.   \n",
    "정수형의 list, tuple 또는 1차원 tensor면 차원에 따른 indices로 나눈다.   \n",
    "* `chunk()`\n",
    "* `dsplit`\n",
    "* `hsplit`\n",
    "* `split`\n",
    "* `vsplit`\n",
    "* `unbind`   \n",
    "Returns a tuple of all slices along a given dimension, already without it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다루지 않은 것\n",
    "\n",
    "- Complex Type 관련된 것  \n",
    "  `torch.complex32`, `Tensor.H` 등\n",
    "- polar 등 다양한 타입\n",
    "- scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 1, 2],\n",
       "        [3, 4, 3, 4],\n",
       "        [1, 2, 1, 2],\n",
       "        [3, 4, 3, 4]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor([[1, 2], [3,4]])\n",
    "data.tile((2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  2],\n",
       "         [ 3,  5]],\n",
       "\n",
       "        [[ 6,  8],\n",
       "         [ 9, 11]]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.arange(0, 12).reshape(2, 2, 3)\n",
    "# torch.take()\n",
    "# torch.take_along_dim()\n",
    "torch.select(data, 1, 0)  # data[:, 0, :]\n",
    "# torch.narrow()\n",
    "torch.masked_select(data, data > 5)\n",
    "torch.index_select(data, 2, torch.tensor([0, 2]))\n",
    "# index_add\n",
    "# gather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2cfc9f92384bea4421cae4ab88ec24798f65a1c0d1ec40acf609fab130197c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
