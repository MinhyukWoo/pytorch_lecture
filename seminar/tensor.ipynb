{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `tensor` 탐구\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def print_tensors(*tensors):\n",
    "    for tensor in tensors:\n",
    "        print(\"=\" * 20)\n",
    "        print(tensor.shape)\n",
    "        print(tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서의 속성\n",
    "\n",
    "- `shape`\n",
    "- `dtype`\n",
    "- `device`\n",
    "- `layout`\n",
    "- `memory_format`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `shape`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(data)\n",
    "print(data.shape)\n",
    "print(data.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dtype`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])'s element data type is torch.float32\n",
      "tensor([4., 5., 6.])'s element data type is torch.float32\n",
      "tensor([7., 8., 9.])'s element data type is torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "float_tensor1 = torch.tensor([1, 2, 3], dtype=torch.float)\n",
    "float_tensor2 = torch.tensor([4, 5, 6], dtype=torch.float32)\n",
    "float_tensor3 = torch.FloatTensor([7, 8, 9])  # Legacy Constructors\n",
    "\n",
    "for tensor in [float_tensor1, float_tensor2, float_tensor3]:\n",
    "    print(f\"{tensor}'s element data type is {tensor.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.int32\n",
      "torch.bool\n",
      "torch.float64\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "# 32-bit floating point\n",
    "print(torch.float)\n",
    "# 32-bit integer (signed)\n",
    "print(torch.int)\n",
    "# Boolean\n",
    "print(torch.bool)\n",
    "\n",
    "# 64-bit floating point\n",
    "print(torch.double)\n",
    "# 64-bit inteber (signed)\n",
    "print(torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`numpy`에서 기본은 `float64`이지만 `torch`에서는 기본이 `float32`이다.  \n",
    "만약 `numpy`와 `tensor`을 동시에 사용한다면 `dtype`에 유의하자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "numpy_data = np.zeros((3, 4), dtype=float)\n",
    "tensor_data = torch.from_numpy(numpy_data)\n",
    "tensor_data.dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `device`\n",
    "\n",
    "- cpu  \n",
    "  `\"cpu\"`를 사용하면 된다.\n",
    "- gpu  \n",
    "  `\"cuda\"`를 사용하면 된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/minhyuk/codes/pytorch_lecture/seminar/tensor.ipynb 셀 9\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/minhyuk/codes/pytorch_lecture/seminar/tensor.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m cpu_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/minhyuk/codes/pytorch_lecture/seminar/tensor.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m gpu_tensor1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor([\u001b[39m4\u001b[39;49m, \u001b[39m5\u001b[39;49m, \u001b[39m6\u001b[39;49m], device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/minhyuk/codes/pytorch_lecture/seminar/tensor.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m gpu_tensor2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m7\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m9\u001b[39m], device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/minhyuk/codes/pytorch_lecture/seminar/tensor.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m [cpu_tensor, gpu_tensor1, gpu_tensor2]:\n",
      "File \u001b[0;32m~/setup/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/cuda/__init__.py:211\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    210\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 211\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    212\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    214\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "cpu_tensor = torch.tensor([1, 2, 3])\n",
    "gpu_tensor1 = torch.tensor([4, 5, 6], device=\"cuda\")\n",
    "gpu_tensor2 = torch.tensor([7, 8, 9], device=torch.device(\"cuda\"))\n",
    "\n",
    "for tensor in [cpu_tensor, gpu_tensor1, gpu_tensor2]:\n",
    "    print(f\"{tensor}'s device is {tensor.device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보통 아래와 같이 사용자 환경에 따라서 동작되도록 설정한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "available_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(available_device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `layout`\n",
    "\n",
    "아직 베타 버전, 바뀔 수 있음\n",
    "\n",
    "> The `torch.layout` class is in beta and subject to change.\n",
    "\n",
    "- `torch.strided`: Dense Tensor\n",
    "- `torch.sparse_coo`: Sparse Tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `memory_format`\n",
    "\n",
    "- `torch.contiguous_format`\n",
    "  > Strides represented by values in decreasing order.\n",
    "- `torch.channels_last`\n",
    "  > strides[0] > strides[2] > strides[3] > strides[1] == 1  \n",
    "  > aka NHWC order.\n",
    "- `torch.preserve_format`\n",
    "  > Used in functions like clone to preserve the memory format of the input tensor. If input tensor is allocated in dense non-overlapping memory, the output tensor strides will be copied from the input. Otherwise output strides will follow torch.contiguous_format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stride: (24, 12, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.arange(0, 24).reshape(1, 2, 3, 4)\n",
    "print(\"stride:\", data.stride())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서 생성하기\n",
    "\n",
    "모두 Deep Copy이다.\n",
    "\n",
    "- `torch.tensor()`  \n",
    "  인자로 전달된 array-like 데이터로 새로운 텐서를 반환한다.\n",
    "- `torch.XXX()`  \n",
    "  인자로 전달된 size의 텐서를 새로 만든다.\n",
    "- `torch.XXX_like()`  \n",
    "  인자로 전달된 텐서와 같은 `size`의 텐서를 새로 만든다.  \n",
    "  `dtype`, `layout`, `device`를 따로 설정하지 않으면 인자로 전달된 텐서와 같은 것을 따른다.\n",
    "- `self.new_XXX()`  \n",
    "  인자로 size를 전달한다.  \n",
    "  `dtype`과 `device`를 유지하는 대신 새로운 텐서를 만든다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "torch.Size([4])\n",
      "tensor([1, 2, 3, 4])\n",
      "====================\n",
      "torch.Size([2, 3])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "====================\n",
      "torch.Size([4])\n",
      "tensor([0, 0, 0, 0])\n",
      "====================\n",
      "torch.Size([3, 2])\n",
      "tensor([[1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data1 = torch.tensor([1, 2, 3, 4])\n",
    "data2 = torch.zeros(2, 3)\n",
    "data3 = torch.zeros_like(data1)\n",
    "data4 = data3.new_ones(3, 2)\n",
    "\n",
    "print_tensors(data1, data2, data3, data4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legacy Constructor\n",
    "\n",
    "- `torch.FloatTensor()`\n",
    "- `torch.cuda.FloatTensor()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "type torch.cuda.FloatTensor not available. Torch not compiled with CUDA enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/minhyuk/codes/pytorch_lecture/seminar/tensor.ipynb 셀 17\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/minhyuk/codes/pytorch_lecture/seminar/tensor.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m legacy_tensor1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor([\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/minhyuk/codes/pytorch_lecture/seminar/tensor.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m legacy_tensor2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mFloatTensor([\u001b[39m4\u001b[39;49m, \u001b[39m5\u001b[39;49m, \u001b[39m6\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/minhyuk/codes/pytorch_lecture/seminar/tensor.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m print_tensors(legacy_tensor1, legacy_tensor2)\n",
      "\u001b[0;31mTypeError\u001b[0m: type torch.cuda.FloatTensor not available. Torch not compiled with CUDA enabled."
     ]
    }
   ],
   "source": [
    "legacy_tensor1 = torch.FloatTensor([1, 2, 3])\n",
    "legacy_tensor2 = torch.cuda.FloatTensor([4, 5, 6])\n",
    "\n",
    "print_tensors(legacy_tensor1, legacy_tensor2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "랜덤 샘플링\n",
    "\n",
    "- `rand`  \n",
    "  $[0, 1)$\n",
    "- `randn`  \n",
    "  $N(0, 1)$\n",
    "- `randint`  \n",
    "  $[low, high)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "torch.Size([2, 3])\n",
      "tensor([[0.4011, 0.1791, 0.7824],\n",
      "        [0.0996, 0.0802, 0.4586]])\n",
      "====================\n",
      "torch.Size([2, 3])\n",
      "tensor([[-1.9820, -0.8721, -1.4842],\n",
      "        [-0.9245,  1.1482, -0.6213]])\n",
      "====================\n",
      "torch.Size([2, 3])\n",
      "tensor([[4, 1, 9],\n",
      "        [1, 1, 5]])\n"
     ]
    }
   ],
   "source": [
    "rand_tensor = torch.rand(2, 3)\n",
    "randn_tensor = torch.randn(2, 3)\n",
    "randint_tensor = torch.randint(10, (2, 3))\n",
    "\n",
    "print_tensors(rand_tensor, randn_tensor, randint_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `full`\n",
    "- `empty`\n",
    "- `ones`\n",
    "- `zeros`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "torch.Size([2, 3])\n",
      "tensor([[3.1400, 3.1400, 3.1400],\n",
      "        [3.1400, 3.1400, 3.1400]])\n",
      "====================\n",
      "torch.Size([2, 3])\n",
      "tensor([[-6.3660e+33,  3.0744e-41, -6.4722e+33],\n",
      "        [ 3.0744e-41,  1.1210e-43,  0.0000e+00]])\n",
      "====================\n",
      "torch.Size([2, 3])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "====================\n",
      "torch.Size([2, 3])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "full_data = torch.full((2, 3), 3.14)\n",
    "empty_data = torch.empty(2, 3)\n",
    "ones_data = torch.ones(2, 3)\n",
    "zeros_data = torch.zeros(2, 3)\n",
    "\n",
    "print_tensors(full_data, empty_data, ones_data, zeros_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다른 데이터에서 tensor로\n",
    "\n",
    "- `torch.tensor()`  \n",
    "  항상 새롭게 복사된다. (deep copy)\n",
    "\n",
    "- `torch.from_numpy()`  \n",
    "  항상 `ndarray`와 같은 메모리를 공유한다.(shallow copy)  \n",
    "  `dtype`, `device`등을 바꿀 수 없다.\n",
    "- `torch.asarray()` or `torch.as_tensor()`  \n",
    "  조건에 따라서 복사될 수도, 메모리를 공유할 수도 있다.  \n",
    "  `dtype`, `device`등을 새로 설정할 수 있다. (이럴 때 복사된다.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "(3,)\n",
      "[10. 10. 10.]\n",
      "====================\n",
      "torch.Size([3])\n",
      "tensor([0., 0., 0.], dtype=torch.float64)\n",
      "====================\n",
      "torch.Size([3])\n",
      "tensor([10., 10., 10.], dtype=torch.float64)\n",
      "====================\n",
      "torch.Size([3])\n",
      "tensor([10., 10., 10.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "numpy_data = np.zeros((3,))\n",
    "tensor_data1 = torch.tensor(numpy_data)\n",
    "tensor_data2 = torch.from_numpy(numpy_data)\n",
    "tensor_data3 = torch.as_tensor(numpy_data)\n",
    "numpy_data += 10\n",
    "print_tensors(numpy_data, tensor_data1, tensor_data2, tensor_data3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor에서 다른 데이터로\n",
    "\n",
    "- `np.array()`  \n",
    "  tensor와 메모리가 공유되지 않는다. (deep copy)\n",
    "\n",
    "- `self.numpy()`  \n",
    "  tensor와 메모리가 공유된다. (shallow copy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "torch.Size([2, 3])\n",
      "tensor([[10., 10., 10.],\n",
      "        [10., 10., 10.]])\n",
      "====================\n",
      "(2, 3)\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "====================\n",
      "(2, 3)\n",
      "[[10. 10. 10.]\n",
      " [10. 10. 10.]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "tensor_data = torch.zeros(2, 3)\n",
    "numpy_data1 = np.array(tensor_data)\n",
    "numpy_data2 = tensor_data.numpy()\n",
    "\n",
    "tensor_data.add_(10)\n",
    "print_tensors(tensor_data, numpy_data1, numpy_data2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연산\n",
    "\n",
    "`self.XXX`는 연산 결과가 반환되지만 self는 반영되지 않는다.  \n",
    "`self.XXX_`는 연산 결과가 in-place로 적용되고 반환도 된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Before===\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "===After===\n",
      "tensor([[12., 12., 12.],\n",
      "        [12., 12., 12.]])\n"
     ]
    }
   ],
   "source": [
    "data = torch.zeros(2, 3)\n",
    "print(\"===Before===\")\n",
    "print(data)\n",
    "data.add_(12)\n",
    "print(\"===After===\")\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape\n",
    "\n",
    "`view`  \n",
    "반드시 같은 데이터를 공유한다. (shallow copy)  \n",
    "조건에 따라서 데이터를 반환할 수 없을 때 `RuntimeError`을 띄운다.\n",
    "\n",
    "`reshape`  \n",
    "같은 데이터를 공유할 때도 있지만, 조건에 따라서 복사해서 제공할 때도 있다.  \n",
    "그러나 Copying과 viewing이 되는 상황을 구분해서 사용하는 것은 좋지 않다.\n",
    "\n",
    "> but you should not depend on the copying vs. viewing behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([1, 2, 3, 4])\n",
      "shape: torch.Size([1, 3, 2, 4])\n",
      "shape: torch.Size([1, 3, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "data = torch.arange(0, 24).reshape(1, 2, 3, 4)\n",
    "viewed_data = data.view(1, 3, 2, 4)\n",
    "reshaped_data = data.view(1, 3, 2, 4)\n",
    "\n",
    "for tensor in [data, viewed_data, reshaped_data]:\n",
    "    print(\"shape:\", tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n",
      "shape: torch.Size([4, 3, 2, 1])\n",
      "shape: torch.Size([1, 3, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "tranposed_data = torch.arange(0, 24).reshape(1, 2, 3, 4).T\n",
    "try:\n",
    "    viewed_data = tranposed_data.view(1, 3, 2, 4)\n",
    "except RuntimeError as error:\n",
    "    print(\"RuntimeError:\", error)\n",
    "reshaped_data = tranposed_data.reshape(1, 3, 2, 4)\n",
    "\n",
    "reshaped_data.add_(10)\n",
    "for tensor in [tranposed_data, reshaped_data]:\n",
    "    print(\"shape:\", tensor.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension swap\n",
    "\n",
    "- `transpose()`또는 `swapdims()`, `swapaxes()`  \n",
    "  두 차원끼리 교환할 때 사용한다.\n",
    "- `permute()`  \n",
    "  원하는 차원들을 교환할 때 사용한다.\n",
    "- `T`  \n",
    "  `permute(n-1, n-2, ..., 0)`과 동일하다.\n",
    "- `view()` and `reshape()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "torch.Size([1, 3, 4])\n",
      "tensor([[[ 0,  3,  6,  9],\n",
      "         [ 1,  4,  7, 10],\n",
      "         [ 2,  5,  8, 11]]])\n",
      "====================\n",
      "torch.Size([4, 1, 3])\n",
      "tensor([[[ 0,  1,  2]],\n",
      "\n",
      "        [[ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8]],\n",
      "\n",
      "        [[ 9, 10, 11]]])\n",
      "====================\n",
      "torch.Size([1, 4, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n"
     ]
    }
   ],
   "source": [
    "data = torch.arange(0, 12).reshape(4, 3, 1)\n",
    "data_t = data.T\n",
    "transposed_data = data.transpose(dim0=1, dim1=2)\n",
    "permuted_data = data.permute(2, 0, 1)\n",
    "\n",
    "\n",
    "print_tensors(data_t, transposed_data, permuted_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "torch.Size([2, 3])\n",
      "tensor([[0, 2, 4],\n",
      "        [1, 3, 5]])\n",
      "====================\n",
      "torch.Size([2, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.arange(0, 6).reshape(3, 2)\n",
    "transposed_data = data.T\n",
    "viewed_data = data.view(2, 3)\n",
    "\n",
    "print_tensors(transposed_data, viewed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension insert\n",
    "\n",
    "- `unsqueeze`\n",
    "- `view` and `reshape`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "torch.Size([1, 6])\n",
      "tensor([[0, 1, 2, 3, 4, 5]])\n",
      "====================\n",
      "torch.Size([1, 6])\n",
      "tensor([[0, 1, 2, 3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "data = torch.arange(0, 6)\n",
    "unsqueezed_data = data.unsqueeze(dim=0)\n",
    "viewed_data = data.view(-1, 6)\n",
    "\n",
    "print_tensors(unsqueezed_data, viewed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension reduction\n",
    "\n",
    "- `ravel`  \n",
    "  항상 뷰가 반환된다. (shallow copy)  \n",
    "  모든 차원에 대해서 flatten한다.\n",
    "- `flatten`  \n",
    "  `numpy`와 다르게 조건에 따라서 원래 객체 또는 뷰가 반환된다.  \n",
    "  flatten할 차원을 지정할 수 있다.\n",
    "- `squeeze`  \n",
    "  size가 1인 차원을 제거한다.  \n",
    "  입력된 차원의 size가 1이면 그 차원을 제거한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "torch.Size([6])\n",
      "tensor([0, 3, 1, 4, 2, 5])\n",
      "====================\n",
      "torch.Size([6])\n",
      "tensor([0, 3, 1, 4, 2, 5])\n",
      "====================\n",
      "torch.Size([3, 2])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n"
     ]
    }
   ],
   "source": [
    "data = torch.arange(0, 6).reshape(1, 2, 3).T\n",
    "raveled_data = data.ravel()\n",
    "flattened_data = data.flatten()\n",
    "squeeze_data = data.squeeze()\n",
    "\n",
    "print_tensors(raveled_data, flattened_data, squeeze_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate\n",
    "\n",
    "모두 deep copy 된다.\n",
    "\n",
    "- `torch.cat()` 또는 `torch.concat()`  \n",
    "  인자로 주어진 차원에서 이어 붙인다.\n",
    "- `torch.stack()`\n",
    "  인자로 주어진 차원을 새로 생성한 뒤, 이어 붙인다.\n",
    "- `vstack` and `row_stack`  \n",
    "  Stack tensors in sequence vertically (row wise).\n",
    "- `hstack`  \n",
    "  Stack tensors in sequence horizontally (column wise).\n",
    "- `dstack`  \n",
    "  Stack tensors in sequence depthwise (along third axis).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "torch.Size([4, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "==========\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0, 1, 2],\n",
      "         [3, 4, 5]],\n",
      "\n",
      "        [[0, 1, 2],\n",
      "         [3, 4, 5]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.arange(0, 6).reshape(2, 3)\n",
    "concat_data = torch.concat([data, data], dim=0)\n",
    "stack_data = torch.stack([data, data], dim=0)\n",
    "\n",
    "for tensor in [concat_data, stack_data]:\n",
    "    print(\"=\" * 10)\n",
    "    print(tensor.shape)\n",
    "    print(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0, 1, 2],\n",
      "         [3, 4, 5]],\n",
      "\n",
      "        [[0, 1, 2],\n",
      "         [3, 4, 5]]])\n",
      "====================\n",
      "torch.Size([1, 4, 3])\n",
      "tensor([[[0, 1, 2],\n",
      "         [3, 4, 5],\n",
      "         [0, 1, 2],\n",
      "         [3, 4, 5]]])\n",
      "====================\n",
      "torch.Size([1, 2, 6])\n",
      "tensor([[[0, 1, 2, 0, 1, 2],\n",
      "         [3, 4, 5, 3, 4, 5]]])\n"
     ]
    }
   ],
   "source": [
    "data = torch.arange(0, 6).reshape(1, 2, 3)\n",
    "vstacked_data = torch.vstack([data, data])\n",
    "hstacked_data = torch.hstack([data, data])\n",
    "dstacked_data = torch.dstack([data, data])\n",
    "\n",
    "print_tensors(vstacked_data, hstacked_data, dstacked_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalar tensor\n",
    "\n",
    "차원이 없이 스칼라 값을 갖는 tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11)\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "scalar = torch.tensor(11)\n",
    "print(scalar)\n",
    "print(scalar.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to\n",
    "\n",
    "조건에 따라서 copy가 될 수도 있고 안 될 수도 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "torch.Size([3])\n",
      "tensor([1, 2, 3], dtype=torch.int32)\n",
      "====================\n",
      "torch.Size([3])\n",
      "tensor([1., 2., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "int_tensor = torch.tensor([1, 2, 3], dtype=torch.int)\n",
    "double_tensor = int_tensor.to(dtype=torch.double)\n",
    "print_tensors(int_tensor, double_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`requires_grad`을 설정해주면 이후의 연산들이 tacking 된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]], requires_grad=True)\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "data1 = torch.zeros(2, 3, requires_grad=True)\n",
    "data2 = torch.zeros(2, 3)\n",
    "data2.requires_grad_(True)\n",
    "\n",
    "print(data1)\n",
    "print(data2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이후 연산할 때마다 연산되는 함수를 텐서에 저장하여 계산 그래프를 구성한다.  \n",
    "속성`self.grad_fn`으로 확인할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(3, 3, requires_grad=True)\n",
    "y = x @ x\n",
    "y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`self.backward()`로 도함수를 계산하게 할 수 있다.  \n",
    "다만, backward하는 텐서가 scalar이거나 매개변수 `grad_output`에 인자를 전달해야 한다.  \n",
    "연산 중간에 있는 텐서의 `grad`를 보고 싶으면 `backward` 전에 `retain_grad`를 호출한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Before Backward=====\n",
      "x: tensor([-1.0000, -0.5000,  0.0000,  0.5000,  1.0000], requires_grad=True)\n",
      "y: tensor([0.2689, 0.3775, 0.5000, 0.6225, 0.7311], grad_fn=<SigmoidBackward0>)\n",
      "z: tensor(2.5000, grad_fn=<AddBackward0>)\n",
      "====After Backward=====\n",
      "x.grad: tensor([0.1966, 0.2350, 0.2500, 0.2350, 0.1966])\n",
      "y.grad: tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "from torch import sigmoid\n",
    "\n",
    "x = torch.linspace(-1, 1, 5, requires_grad=True)\n",
    "y = sigmoid(x)\n",
    "z = sum(y)\n",
    "print(\"====Before Backward=====\")\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)\n",
    "print(\"z:\", z)\n",
    "y.retain_grad()\n",
    "z.backward()\n",
    "print(\"====After Backward=====\")\n",
    "print(\"x.grad:\", x.grad)\n",
    "print(\"y.grad:\", y.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Before Backward=====\n",
      "x: tensor([-5.0000, -2.5000,  0.0000,  2.5000,  5.0000], requires_grad=True)\n",
      "w: tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.,  9.],\n",
      "        [10., 11., 12., 13., 14.]], requires_grad=True)\n",
      "b: tensor([0., 1., 2.], requires_grad=True)\n",
      "y: tensor([25., 26., 27.], grad_fn=<AddBackward0>)\n",
      "z: tensor(78., grad_fn=<AddBackward0>)\n",
      "====After Backward=====\n",
      "x.grad: tensor([15., 18., 21., 24., 27.])\n",
      "w.grad: tensor([[-5.0000, -2.5000,  0.0000,  2.5000,  5.0000],\n",
      "        [-5.0000, -2.5000,  0.0000,  2.5000,  5.0000],\n",
      "        [-5.0000, -2.5000,  0.0000,  2.5000,  5.0000]])\n",
      "b.grad: tensor([1., 1., 1.])\n",
      "y.grad: tensor([1., 1., 1.])\n",
      "====검산====\n",
      "x.grad: True\n",
      "w.grad: True\n"
     ]
    }
   ],
   "source": [
    "x = torch.linspace(-5, 5, 5, dtype=torch.float, requires_grad=True)\n",
    "w = torch.arange(0, 15, dtype=torch.float).reshape(3, 5).requires_grad_(True)\n",
    "b = torch.arange(0, 3, dtype=torch.float, requires_grad=True)\n",
    "y = w @ x + b\n",
    "z = sum(y)\n",
    "print(\"====Before Backward=====\")\n",
    "print(\"x:\", x)\n",
    "print(\"w:\", w)\n",
    "print(\"b:\", b)\n",
    "print(\"y:\", y)\n",
    "print(\"z:\", z)\n",
    "y.retain_grad()\n",
    "z.backward()\n",
    "print(\"====After Backward=====\")\n",
    "print(\"x.grad:\", x.grad)\n",
    "print(\"w.grad:\", w.grad)\n",
    "print(\"b.grad:\", b.grad)\n",
    "print(\"y.grad:\", y.grad)\n",
    "print(\"====검산====\")\n",
    "print(\"x.grad:\", torch.equal(x.grad, w.T @ y.grad))\n",
    "print(\"w.grad:\", torch.equal(w.grad, y.grad.unsqueeze(dim=1) @ x.unsqueeze(0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stops autograd from tracking history on Tensor\n",
    "\n",
    "- `with torch.no_grad()`\n",
    "- `self.detach()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([-1.0000, -0.5000,  0.0000,  0.5000,  1.0000], requires_grad=True)\n",
      "y: tensor([0.2689, 0.3775, 0.5000, 0.6225, 0.7311])\n",
      "z: tensor(2.5000)\n",
      "RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    x = torch.linspace(-1, 1, 5, requires_grad=True)\n",
    "    y = sigmoid(x)\n",
    "    z = sum(y)\n",
    "    print(\"x:\", x)\n",
    "    print(\"y:\", y)\n",
    "    print(\"z:\", z)\n",
    "    try:\n",
    "        z.backward()\n",
    "    except RuntimeError as error:\n",
    "        print(\"RuntimeError:\", error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([-1.0000, -0.5000,  0.0000,  0.5000,  1.0000], requires_grad=True)\n",
      "y: tensor([0.2689, 0.3775, 0.5000, 0.6225, 0.7311])\n",
      "z: tensor(2.5000)\n"
     ]
    }
   ],
   "source": [
    "x = torch.linspace(-1, 1, 5, requires_grad=True)\n",
    "y = sigmoid(x)\n",
    "y.detach_()\n",
    "z = sum(y)\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)\n",
    "print(\"z:\", z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e8b8adcd60e3125d3846633b1286e68149d6edb0e860e6f97e15020191dc869"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
